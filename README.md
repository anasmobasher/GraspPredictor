# GraspPredictor
* GraspPredictor provides a neural network-based model that predicts feasible grasps for a given scene.
* Used to evaluate the feasibility of training grasp predictors on synthetic data generated by NeRF (a part of the thesis project)

* GraspPredictor is a neural network that can predict the feasibil grasps of a given RGBD image of a scene. It is trained in a supervised manner. 

## Datasets
* Dataset consists of a different scene of different objects with its corresponding grasp quality map which can be generated manually or using an automatic labeling approach. (automatic labeling approach will be added).

## Methodology 
* Will be added 

## Results 

4 instances of the GraspNet are trained on different datasets:
1. NeRF-Synth Dataset: This dataset is generated using a combination of Object-NeRFs and the corresponding Background-NeRF. It consists of novel views of the input scene.

2. Real-Only Dataset: This dataset comprises real RGB-D images recorded by RealSense camera, that are corresponding to the generated images in NeRF-Synth Dataset, and it is not used in training our model. However, this data is labeled using the recorded images and the corresponding background images rendered by Background-NeRF as we lack aligned real background images. 

3. Camera-Augmented Dataset: This dataset includes the NeRF-Synth Dataset along with additional novel views rendered using different camera intrinsics. It is labeled similarly to the labeling of the NeRF-Synth Dataset. 

4. Novel-Scene Dataset: This dataset comprises the Camera-Augmented Dataset and additional novel views of novel synthetic scenes generated using the proposed method for extracting single objects. It is labeled similarly to the labeling of the NeRF-Synth Dataset.

### Evaluation dataset
* RealSense: This dataset comprises images collected using the same camera and setup as used for collecting the training dataset. It is specifically designed to evaluate the capabilities of the network to generalize to previously unseen objects within a similar environment to that used for collecting the training dataset. The scenes in this dataset include only separated objects, and the main objective is to assess the network’s ability to grasp and recognize objects it has not encountered during training.

* Zivid: This dataset is collected using a different camera, specifically the Zivid 2 camera, which provides a high resolution of 1920×1200. The scene in this dataset contains a completely different background and includes a larger number of objects. The scenes in this dataset are designed to resemble a typical bin-picking application, where objects are placed in a bin to be picked. The dataset encompasses both, seen and unseen objects with different configurations, such as separated, stacked, and overlapped objects. With this dataset we evaluate how well the networks generalize to new environments, as well as different cameras and backgrounds.

### Evaluation Metrics
* Grasp Success, Success: The percentage of feasible grasps among the predicted grasps, where feasible grasps require non-zero grasp quality values. A successful grasp means that the predicted grasp is considered valid and likely to succeed.
*  Grasp-Quality, Quality: The average ground-truth grasp quality of feasible grasps, which ranges between 0 and 1. This metric gives an indication of how well the predicted grasps align with the actual quality of the grasping action.
*  Object clearing rate, Object: The percentage of objects for which grasps are detected. This metric counts the proportion of objects in the scene for which the grasp prediction network successfully detects valid grasps.

![alt text](https://github.com/anasmobasher/GraspPredictor/blob/main/docs/grasp/results.png?raw=true)
